NeuralNetwork implementation with an arbitrary number of inputs dimensions, hidden and output neurone using Relu and Softmax activation functions

To execute the code, in a terminal  enter:
python3 Main.py -test_percent 20 -path circles.txt -sizes 2 6 2 -lrate 0.001 -epochs 1000

sizes : 
dimension of the input,
number of neurones per hidden layer, 
number of output neurones.

lrate is the learning rate
